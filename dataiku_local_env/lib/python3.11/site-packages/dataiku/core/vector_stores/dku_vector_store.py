import enum
import logging
import os
import os.path as osp
from typing import TYPE_CHECKING, Dict, List

import dataiku
from dataiku.base.utils import package_is_at_least_no_import
from dataiku.core import dkujson

if TYPE_CHECKING:
    from langchain_core.vectorstores import VectorStoreRetriever

logger = logging.getLogger(__name__)


RECORD_MANAGER_FILENAME = "record_manager_cache.sqlite"


class SearchType(enum.Enum):
    MMR = "mmr"
    SIMILARITY = "similarity"


class RecordManagerCleanupMode(enum.Enum):
    FULL = "full"  # deletes docs that are not included in the docs currently being indexed
    INCREMENTAL = "incremental"  # deletes previous versions of modified docs
    NONE = None  # does not delete anything


class UpdateMethod(enum.Enum):
    SMART_OVERWRITE = "SMART_OVERWRITE"  # called Smart Sync in the UI; requires a sourceIdColumn in the KB
    SMART_APPEND = "SMART_APPEND"  # called Upsert in the UI; requires a sourceIdColumn in the KB
    OVERWRITE = "OVERWRITE"
    APPEND = "APPEND"


class DkuVectorStore:
    exec_folder = None

    def __init__(self, kb, exec_folder):
        """
        :param kb: The contents of kb.json (dict)
        :param exec_folder: Vector Store location on disk:
                            /!/ exec_folder can be either :
                                - the KB root folder (containing all the versions)- when called from the clear server
                                - or the current KB version subfolder - when called from the embedding recipe & rag query server.
        :type exec_folder: string|None
        """
        self.kb = kb
        self.exec_folder = exec_folder
        self.source_id_column = kb.get("sourceIdColumn")  # Can be None, if not in a smart update mode

    @staticmethod
    def _get_mmr_args(rag_settings: Dict) -> Dict:
        use_mmr = rag_settings.get("useMMR", False)
        if not use_mmr:
            return {}
        return {"fetch_k": rag_settings.get("mmrK", 20), "lambda_mult": rag_settings.get("mmrDiversity", 0.25)}

    @staticmethod
    def _get_search_kwargs(rag_settings: Dict) -> Dict:
        # By default, the k param (nb of documents to return) is passed as part of the search kwargs
        return {
            "k": rag_settings.get("maxDocuments", 4),
            **DkuVectorStore._get_mmr_args(rag_settings)
        }

    @staticmethod
    def _get_retriever_kwargs(rag_settings: Dict) -> Dict:
        # By default no retriever kwargs are needed
        return {}

    @staticmethod
    def _get_db_kwargs(rag_settings: Dict) -> Dict:
        # By default no db kwargs are needed
        return {}

    @staticmethod
    def _get_search_type(rag_settings: Dict) -> str:
        use_mmr = rag_settings.get("useMMR", False)
        return SearchType.MMR.value if use_mmr else SearchType.SIMILARITY.value

    def as_retriever(self, embeddings, rag_settings: Dict) -> "VectorStoreRetriever":
        return self.get_db(embeddings, **self._get_db_kwargs(rag_settings)).as_retriever(
            search_type=self._get_search_type(rag_settings),
            search_kwargs=self._get_search_kwargs(rag_settings),
            **self._get_retriever_kwargs(rag_settings)
        )

    def transform_document_before_load(self, document):
        """
        Hook for a vectorstore to rewrite the document at load time.
        Used to reformat security groups for Chroma
        """
        return document

    def add_security_filter(self, search_kwargs: Dict, caller_groups : List):
        raise Exception("Security filter not implemented on this vector store")

    def add_filter(self, search_kwargs: Dict, filter : Dict):
        raise Exception("Explicit filter not implemented on this vector store")

    def load_documents(self, documents_loader, embeddings, update_method=UpdateMethod.OVERWRITE):
        """
        Load the given documents into the vector store, with the provided update method.

        :type documents_loader: dataiku.langchain.VectorStoreLoader
        :type embeddings: dataiku.langchain.dku_embeddings.DKUEmbeddings
        :param update_method: One of:
            UpdateMethod.SMART_OVERWRITE: Sync the vector store with the documents, using langchain RecordManager. Requires a sourceIdColumn set on the KB.
            UpdateMethod.SMART_APPEND: Upsert the documents into the vector store, using langchain RecordManager. Requires a sourceIdColumn set on the KB.
            UpdateMethod.OVERWRITE (default): Clear the vector store and add the documents
            UpdateMethod.APPEND: Add the documents to the vector store
        :type update_method: UpdateMethod

        :return: The new version of the vector store, equivalent to calling self.get_db(embeddings)
        :rtype: langchain_community.vectorstores.VectorStore
        """
        if update_method == UpdateMethod.SMART_OVERWRITE:
            return self._managed_overwrite_documents(documents_loader, embeddings)
        elif update_method == UpdateMethod.SMART_APPEND:
            return self._managed_append_documents(documents_loader, embeddings)
        elif update_method == UpdateMethod.OVERWRITE:
            return self._overwrite_documents(documents_loader, embeddings)
        elif update_method == UpdateMethod.APPEND:
            return self._append_documents(documents_loader, embeddings)
        else:
            raise ValueError("Unknown knowledge bank update method %s" % update_method)

    def _overwrite_documents(self, documents_loader, embeddings):
        logger.info("Adding documents to the vector store, in overwrite mode")
        if self.source_id_column is not None:
            raise ValueError("Source ID column cannot be used with overwrite mode")
        self.clear()
        vectorstore_db = self.get_db(embeddings, allow_creation=True)
        self._add_documents(vectorstore_db, documents_loader)
        return vectorstore_db

    def _managed_overwrite_documents(self, documents_loader, embeddings):
        logger.info("Adding documents to the vector store, in managed overwrite mode")
        if not self.source_id_column:
            raise ValueError("Source ID column is required when using smart sync mode")
        vectorstore_db = self.get_db(embeddings, allow_creation=True)
        self._index_documents(documents_loader, vectorstore_db, cleanup_mode=RecordManagerCleanupMode.FULL, source_id_key=self.source_id_column)
        return vectorstore_db

    def _append_documents(self, documents_loader, embeddings):
        logger.info("Adding documents to the vector store, in append mode")
        if self.source_id_column is not None:
            raise ValueError("Source ID column cannot be used with upsert mode")
        vectorstore_db = self.get_db(embeddings, allow_creation=True)
        self._add_documents(vectorstore_db, documents_loader)
        return vectorstore_db

    def _managed_append_documents(self, documents_loader, embeddings):
        logger.info("Adding documents to the vector store, in managed append mode")
        if not self.source_id_column:
            raise ValueError("Source ID column is required when using upsert mode")
        vectorstore_db = self.get_db(embeddings, allow_creation=True)
        self._index_documents(documents_loader, vectorstore_db, cleanup_mode=RecordManagerCleanupMode.INCREMENTAL, source_id_key=self.source_id_column)
        return vectorstore_db

    def _add_documents(self, vectorstore_db, documents_loader):
        total = 0
        nb_batches = 0
        for documents in documents_loader.lazy_load():
            for document in documents:
                self.transform_document_before_load(document)

            total += len(documents)
            nb_batches += 1
            vectorstore_db.add_documents(documents)
            logger.info(f"Added a batch of {len(documents)} documents to the vector store ({total} added so far)")
        if total > 0:
            logger.info(f"Added {total} documents to the vector store in {nb_batches} batches")

    def _index_documents(self, documents_loader, vectorstore_db, cleanup_mode, source_id_key, batch_size=100):  # 100 is the default batch size used by RecordManager
        from langchain.indexes import SQLRecordManager, index

        batch_size = self._check_batch_size_against_sqlite_version(batch_size)

        record_manager_file_path = os.path.join(self.exec_folder, RECORD_MANAGER_FILENAME)
        record_manager_already_exists = os.path.exists(record_manager_file_path)
        if record_manager_already_exists:
            logger.info(f"Using existing record manager: {record_manager_file_path}")
        else:
            logger.info(f"Creating new record manager: {record_manager_file_path}")
        record_manager = SQLRecordManager(namespace="record_manager_namespace", db_url=f"sqlite:///{record_manager_file_path}")
        if not record_manager_already_exists:
            record_manager.create_schema()

        logger.info(f"Indexing documents, with cleanup_mode={cleanup_mode}, source_id_key={source_id_key}, batch_size={batch_size}")

        def iter_transformed_documents():
            for doc in documents_loader.iter_documents():
                ret = self.transform_document_before_load(doc)
                yield ret

        index_info = index(
            iter_transformed_documents(),
            record_manager,
            vectorstore_db,
            cleanup=cleanup_mode.value,
            source_id_key=source_id_key,
            batch_size=batch_size,
        )
        logger.info(f"Vector store indexing done {index_info}")

    @staticmethod
    def _check_batch_size_against_sqlite_version(batch_size):
        # Older versions of sqlite have a lower SQLITE_MAX_VARIABLE_NUMBER, which causes issues for RecordManager when the batch size is too high
        try:
            import sqlite3
            safe_batch_size = 190  # RecordManager uses 5 variables per row, and SQLITE_MAX_VARIABLE_NUMBER=999 on sqlite versions before 3.32
            if batch_size > safe_batch_size and sqlite3.sqlite_version_info < (3, 32, 0):
                logger.warning(f"Reducing the RecordManager batch size to {safe_batch_size} because of an older version of sqlite. " +
                                "To use larger batch sizes, please upgrade to sqlite3 >= 3.32")
                batch_size = safe_batch_size
        except:
            pass  # ignore missing sqlite in case future versions of RecordManager use a different sql engine

        return batch_size

    def get_db(self, embeddings, allow_creation=False, **kwargs):
        """
        :type embeddings: dataiku.langchain.dku_embeddings.DKUEmbeddings
        :param allow_creation: whether creating new resources if the db doesn't exist is allowed.
        :type allow_creation: boolean
        :rtype: langchain_community.vectorstores.VectorStore
        """
        raise NotImplementedError()

    def clear(self):
        """ Clear all data in the knowledge bank - used only from the clear server for now"""
        raise NotImplementedError()

    def clear_record_manager(self, folder_path):
        record_manager_file_path = os.path.join(folder_path, RECORD_MANAGER_FILENAME)
        if os.path.isfile(record_manager_file_path):
            os.remove(record_manager_file_path)

    def get_vector_size(self):
        # create a dummy embedding query via the public api to retrieve the embedding size no matter which llm was selected.
        if not self.kb.get("embeddingLLMId"):
            raise ValueError("An embedding model must be selected")

        project_handle = dataiku.api_client().get_default_project()
        llm_model = project_handle.get_llm(self.kb["embeddingLLMId"])

        query = llm_model.new_embeddings()
        query.add_text("This is just a dummy query to get embeddingSize")
        model_embedding_size = len(query.execute().get_embeddings()[0])

        logger.info("Retrieved vector size for LLM id {}: {}".format(self.kb["embeddingLLMId"], model_embedding_size))
        return model_embedding_size


class DkuLocalVectorStore(DkuVectorStore):

    def __init__(self, kb, exec_folder, collection_name):
        super(DkuLocalVectorStore, self).__init__(kb, exec_folder)
        self.collection_name = collection_name

    def clear(self):
        """ Delete all the KB versioned data
        /!/ exec_folder here is expected to be KB root folder (containing all the KB versions)
        """
        self.clear_files(self.exec_folder)
        self.clear_record_manager(self.exec_folder)

        if os.path.isdir(os.path.join(self.exec_folder, "versions")):
            for versionDir in os.listdir(os.path.join(self.exec_folder, "versions")):
                version_folder_path = os.path.join(self.exec_folder, "versions", versionDir)
                if os.path.isdir(version_folder_path):
                    self.clear_files(version_folder_path)
                    self.clear_record_manager(version_folder_path)

        logger.info("Cleared local vector store at {}".format(self.exec_folder))

    def clear_files(self, folder_path):
        raise NotImplementedError()


class DkuRemoteVectorStore(DkuVectorStore):

    def __init__(self, kb, exec_folder, bulk_size=1000):
        super(DkuRemoteVectorStore, self).__init__(kb, exec_folder)
        self.index_name = None

        index_name = kb.get("resolvedIndexName")
        if index_name is None:
            if kb.get("indexName"):
                raise ValueError("The index name variables could not be resolved.")
            else:
                raise ValueError("You must provide a value for the Knowledge Bank index name.")
        self.set_index_name(index_name)

        self.type = kb["vectorStoreType"]
        connection = self.get_connection(kb)
        self.init_connection(connection)
        self.bulk_size = bulk_size
        bulk_size_from_connection_params = self.get_bulk_size_from_connection_params(connection.get_info()['params'])
        if bulk_size_from_connection_params:
            self.bulk_size = bulk_size_from_connection_params

    def get_bulk_size_from_connection_params(self, connection_params):
        for property in connection_params.get('dkuProperties', []):
            if property['name'] == "dku.embedding.bulkSize":
                try:
                    bulk_size = int(property['value'])
                    logger.info("Set custom bulk size: {}".format(bulk_size))
                    return bulk_size
                except ValueError:
                    logger.warning("Ignoring invalid value for bulk size connection property (expected an integer): {}".format(property['value']))

    def _load_remote_resources_references(self, exec_folder: str) -> Dict:
        """ Loads the references to pre-existing remote-resources for the selected vector type
            :exec_folder: folder from which loading the remote_resource references file. (the kb version folder)
            :return dict containing the references to remote resources (ids/names required to manage their lifecycle)
        """
        if exec_folder is not None:
            filepath = osp.join(exec_folder, "{}_remote_resources.json".format(self.type))  # todo double check read/write permissions on this file
            if osp.exists(filepath):
                logger.info("Loading remote resources references from {}".format(filepath))
                return dkujson.load_from_filepath(filepath)
        return {}

    def _dump_remote_resources_references(self, remote_resources_ref: Dict, exec_folder: str):
        """ Dump the references to remote resources used for this KB version
            :param remote_resources_ref: dict containing the references to remote resources (ids/names required to manage their lifecycle)
            :param exec_folder: folder in which we store the remote_resource references file. (the kb version folder)
        """
        filepath = osp.join(exec_folder, "{}_remote_resources.json".format(self.type))
        logger.info("Dumping remote resources references to {}".format(filepath))
        dkujson.dump_to_filepath(filepath, remote_resources_ref)

    def set_index_name(self, index_name):
        self.index_name = index_name

    def init_connection(self, connection):
        raise NotImplementedError()

    def clear(self):
        """ Delete current index & any local files in exec_folder & subfolders.
            /!/ exec_folder can be the KB root folder (containing all the KB versions) or one of the versions folders.
        """
        self.clear_index()   # to improve: if index name changed between 2 kb build, this will only clean remote resources with current index name.

        if self.exec_folder is not None:
            self.clear_record_manager(self.exec_folder)

            if os.path.isdir(os.path.join(self.exec_folder, "versions")):
                for versionDir in os.listdir(os.path.join(self.exec_folder, "versions")):
                    version_folder_path = os.path.join(self.exec_folder, "versions", versionDir)
                    if os.path.isdir(version_folder_path):
                        self.clear_record_manager(version_folder_path)

    def clear_index(self):
        """ Clear remote resources with current index name & kb version"""
        raise NotImplementedError()

    def get_connection(self, kb):
        connection = dataiku.api_client().get_connection(kb['connection'])
        if connection is None:
            raise ValueError("You must provide a connection to be used with the knowledge bank.")
        connection_info = connection.get_info()
        if "params" not in connection_info:
            raise Exception("You lack the permission to read the details of the connection " + kb['connection'] + ".")
        return connection

    def get_batches(self, documents):
        return (documents[i:min(len(documents), i + self.bulk_size)] for i in range(0, len(documents), self.bulk_size))

    def ensure_documents_are_indexed(self):
        # nothing to do for most implementations
        pass

    def _batch_add_documents(self, documents_loader, embeddings):
        vectorstore_db = self.get_db(embeddings, allow_creation=True)
        total = 0
        nb_batches = 0
        for documents in documents_loader.lazy_load():
            for document in documents:
                self.transform_document_before_load(document)
            for batch in self.get_batches(documents):
                total += len(batch)
                nb_batches += 1
                vectorstore_db.add_documents(batch)
                logger.info(f"Added a batch of {len(batch)} documents to the vector store ({total} added so far)")
        if total > 0:
            logger.info(f"Added {total} documents to the vector store in {nb_batches} batches")
        self.ensure_documents_are_indexed()
        return vectorstore_db

    def _overwrite_documents(self, documents_loader, embeddings):
        logger.info("Adding documents to the remote vector store, in overwrite mode")
        self.clear()  # note that if index name changed in between 2 kb builds some remote resources will not be removed here.
        return self._batch_add_documents(documents_loader, embeddings)

    def _append_documents(self, documents_loader, embeddings):
        logger.info("Adding documents to the remote vector store, in append mode")
        return self._batch_add_documents(documents_loader, embeddings)

    def _index_documents(self, documents_loader, vectorstore_db, cleanup_mode, source_id_key, batch_size=None):
        if batch_size is None:
            batch_size = self.bulk_size
        super(DkuRemoteVectorStore, self)._index_documents(documents_loader, vectorstore_db, cleanup_mode, source_id_key, batch_size)
        self.ensure_documents_are_indexed()


class VectorStoreFactory:

    @staticmethod
    def get_vector_store(kb, exec_folder):
        """
        :param kb: The contents of kb.json
        :type kb: dict
        :param exec_folder: Vector Store location on disk
        :type exec_folder: string|None
        :rtype: DkuVectorStore
        """
        vector_store_type = kb["vectorStoreType"]

        if vector_store_type == "FAISS":
            from dataiku.core.vector_stores.faiss_vector_store import FAISSVectorStore
            return FAISSVectorStore(kb, exec_folder)

        elif vector_store_type == "PINECONE":
            return VectorStoreFactory.get_correct_pinecone_vectorstore(kb, exec_folder)

        elif vector_store_type == "AZURE_AI_SEARCH":
            from dataiku.core.vector_stores.azureaisearch_vector_store import AzureAISearchVectorStore
            return AzureAISearchVectorStore(kb, exec_folder)

        elif vector_store_type == "VERTEX_AI_GCS_BASED":
            from dataiku.core.vector_stores.vertexai_vector_store import VertexAiVectorStore
            return VertexAiVectorStore(kb, exec_folder)

        elif vector_store_type == "ELASTICSEARCH":
            return VectorStoreFactory.get_correct_elasticsearch_vectorstore(kb, exec_folder)

        elif vector_store_type == "CHROMA":
            from dataiku.core.vector_stores.chroma_vector_store import ChromaVectorStore
            return ChromaVectorStore(kb, exec_folder)

        elif vector_store_type == "QDRANT_LOCAL":
            from dataiku.core.vector_stores.qdrant_local_vector_store import QDrantLocalVectorStore
            return QDrantLocalVectorStore(kb, exec_folder)

        else:
            raise NotImplementedError("Requested vector store type invalid: {}".format(vector_store_type))

    @staticmethod
    def needs_local_path(kb):
        vector_store_type = kb["vectorStoreType"]

        if vector_store_type in {"PINECONE", "ELASTICSEARCH", "AZURE_AI_SEARCH"} and not kb.get("sourceIdColumn"):
            return False

        return True

    @staticmethod
    def get_correct_pinecone_vectorstore(kb, exec_folder):
        if package_is_at_least_no_import("pinecone-client", "3.0"):
            from dataiku.core.vector_stores.pinecone_v3_vector_store import PineconeV3VectorStore
            return PineconeV3VectorStore(kb, exec_folder)
        else:
            from dataiku.core.vector_stores.pinecone_v2_vector_store import PineconeV2VectorStore
            return PineconeV2VectorStore(kb, exec_folder)

    @staticmethod
    def get_correct_elasticsearch_vectorstore(kb, exec_folder):
        # This is an ugly hack to auto-detect the distribution of the connection
        # it will be replaced once we have settled for the proper way for the user to declare
        # and configure OpenSearch connections
        from elasticsearch import UnsupportedProductError
        try:
            from dataiku.core.vector_stores.elasticsearch_vector_store import ElasticSearchVectorStore
            vs = ElasticSearchVectorStore(kb, exec_folder)
            logger.info("Using ElasticSearch vector store implementation.")
            return vs
        except UnsupportedProductError as e:
            distribution = e.body.get("version", {}).get("distribution", "")
            is_opensearch = distribution == "opensearch"
            # AWS OpenSearch Managed Cluster in compatibility mode doesn't report distribution
            if not is_opensearch and "opensearch" in e.body.get("tagline", "").lower():
                is_opensearch = True
            if is_opensearch:
                from dataiku.core.vector_stores.opensearch_vector_store import OpenSearchVectorStore
                logger.info("Using OpenSearch vector store implementation.")
                return OpenSearchVectorStore(kb, exec_folder)
            else:
                raise UnsupportedProductError("You need to use an ElasticSearch version >= 7", e.meta, e.body)
