import enum
import logging
import time
from typing import Dict

import pandas as pd
from azure.core import credentials
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import SearchableField, SearchField, SearchFieldDataType, SimpleField
from langchain_community.vectorstores import azuresearch

from dataiku.core.vector_stores.dku_vector_store import DkuRemoteVectorStore, logger

logging.getLogger("azure").setLevel(logging.WARNING)


SEMANTIC_CONFIGURATION_NAME = "DKU__SEMANTIC_CONFIG"

class AzureBearerTokenCredential(credentials.TokenCredential):
    def __init__(self, access_token_getter):
        self._token_getter = access_token_getter

    def get_token(self, *scopes: str, claims=None, tenant_id=None, enable_cae=False, **kwargs) -> credentials.AccessToken:
        return self._token_getter(scopes, claims, tenant_id, enable_cae, **kwargs)


# monkey patch the class so that it can be instantiated:
credentials.TokenCredential = AzureBearerTokenCredential


class AzureAISearchType(enum.Enum):
    SIMILARITY = "similarity"
    HYBRID = "hybrid"
    SEMANTIC_HYBRID = "semantic_hybrid"


class AzureAISearchVectorStore(DkuRemoteVectorStore):

    AZUREAISEARCH_RESOURCE_URL_FORMAT = "https://{}.search.windows.net"
    DEFAULT_OAUTH_SCOPE = "https://search.azure.com/.default"

    def __init__(self, kb, exec_folder):
        self.metadata_columns_schema = kb.get("metadataColumnsSchema", [])
        self.index_client = None
        self.azure_search_key = None
        self.azure_ai_search_url = None
        self.azure_credential = None  # to connect to Azure SearchIndexClient on our side
        self.auth_kwargs = {}  # for langchain (also connecting to the SearchIndexClient but internally)
        super().__init__(kb, exec_folder)

    def init_connection(self, connection):
        connection_infos = connection.get_info()
        connection_params = connection_infos.get_params()
        self.azure_ai_search_url = self.build_azure_ai_search_resource_url(connection_params['resourceName'])

        if connection_params["authType"] == "OAUTH2_APP":
            logger.info("Initiating the connection to Azure AI Search with Oauth ({})".format(
                connection_infos["credentialsMode"])
            )
            # Global & Per user oauth: retrieve the credentials resolved in the Java part
            resolved_access_token = connection_infos.get_oauth2_credential()['accessToken']
            self.azure_credential = AzureBearerTokenCredential(
                lambda *scopes, **kwargs: credentials.AccessToken(resolved_access_token, int(time.time()) + 3600)
            )

            self.auth_kwargs = {
                "azure_ad_access_token": resolved_access_token,
                "azure_search_key": None  # required: as of today azure_search_key isn't optional nor has default value in langchain api
            }

        elif connection_params["authType"] == "API_KEY":
            logger.info("Initiating the connection to Azure AI Search with Api Key")

            self.azure_search_key = connection_params['apiKey']
            self.azure_credential = credentials.AzureKeyCredential(self.azure_search_key)

            self.auth_kwargs = {"azure_search_key": self.azure_search_key}

        else:
            raise Exception("Unsupported authorization type {} for Azure AI Search connection".format(connection_params["authType"]))

        # Create a search index client once to be able to clear the index when required
        self.index_client = SearchIndexClient(endpoint=self.azure_ai_search_url, credential=self.azure_credential)

    @classmethod
    def build_azure_ai_search_resource_url(cls, name_or_url):
        # (http unsupported by the api)
        return name_or_url if name_or_url.startswith("https://") else cls.AZUREAISEARCH_RESOURCE_URL_FORMAT.format(name_or_url)

    @staticmethod
    def _get_search_kwargs(rag_settings: Dict) -> Dict:
        # Neither mmr nor k kwargs are supported for azure AI retriever (search kwargs)
        return {}

    @staticmethod
    def _get_retriever_kwargs(rag_settings: Dict) -> Dict:
        # For azure AI the k is defined on the retriever kwargs directly
        return {"k": rag_settings.get("maxDocuments", 4)}

    @staticmethod
    def _get_search_type(rag_settings: Dict) -> str:
        hybrid_search = rag_settings.get("useHybridSearch", False)
        advanced_reranking = rag_settings.get("useAdvancedReranking", False)
        # Assuming that if hybrid_search isn't enabled then advanced_reranking is irrelevant
        if hybrid_search:
            if advanced_reranking:
                return AzureAISearchType.SEMANTIC_HYBRID.value
            else:
                return AzureAISearchType.HYBRID.value
        else:
            return AzureAISearchType.SIMILARITY.value

    def set_index_name(self, index_name):
        self.index_name = index_name.lower()

    def clear_index(self):
        self.index_client.delete_index(self.index_name)

    def get_db(self, embeddings, allow_creation=False, **kwargs):
        # todo should check if index already exist to raise an error if allow_creation=false first (langchain always create it by default if unfound)

        fields_from_metadata = [
            SimpleField(
                name=schema_column["name"],
                type=get_azure_data_type(schema_column["type"], schema_column["name"]),
                filterable=True,
                searchable=False,  # don't want these fields used in hybrid search
            )
            for schema_column in self.metadata_columns_schema
        ]

        db = azuresearch.AzureSearch(
            azure_search_endpoint=self.azure_ai_search_url,
            index_name=self.index_name,
            embedding_function=embeddings,
            semantic_configuration_name=SEMANTIC_CONFIGURATION_NAME,
            fields=self.get_required_fields() + fields_from_metadata,
            **self.auth_kwargs
        )
        return db

    def transform_document_before_load(self, document):
        new_meta = {}
        for k, v in document.metadata.items():
            if pd.isna(v):
                new_meta[k] = None
            else:
                new_meta[k] = v
        document.metadata = new_meta
        return document

    def get_required_fields(self):
        # These fields are required by Azure AI search
        # From the AzureSearch langchain wrapper, see the definition of `default_fields` in `AzureSearch.__init__`
        # https://python.langchain.com/api_reference/_modules/langchain_community/vectorstores/azuresearch.html
        return [
            SimpleField(
                name="id",
                type=SearchFieldDataType.String,
                key=True,
                filterable=True,
            ),
            SearchableField(
                name="content",
                type=SearchFieldDataType.String,
                searchable=True,
            ),
            SearchField(
                name="content_vector",
                type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                searchable=True,
                vector_search_dimensions=self.get_vector_size(),
                vector_search_profile_name="myHnswProfile",  # a profile with this name is created by the AzureSearch langchain wrapper
            ),
            SearchableField(
                name="metadata",
                type=SearchFieldDataType.String,
                searchable=True,
            )
        ]

def get_azure_data_type(dss_storage_type, column_name) -> SearchFieldDataType:
    # Maps a dss storage type to an Azure SearchFieldDataType
    # The list of DSS storage types is in the Java class: com.dataiku.dip.datasets.Type
    # The list of Azure data types: https://learn.microsoft.com/en-us/dotnet/api/azure.search.documents.indexes.models.searchfielddatatype?view=azure-dotnet#properties
    if dss_storage_type == "string":
        return SearchFieldDataType.String
    elif dss_storage_type in ["date", "dateandtime", "dateonly"]:
        return SearchFieldDataType.String  # Not using Azure's date type because this would require converting the data into Azure's format
    elif dss_storage_type in ["geopoint", "geometry"]:
        return SearchFieldDataType.String # Not using Azure's geometry type because this would require converting the data into Azure's format
    elif dss_storage_type in ["array", "map", "object"]:
        # TODO @azureai-filtering Possibly add schema for complex fields (NB: Needs Azure API version >= 2019-05-06)
        #  https://learn.microsoft.com/en-gb/azure/search/search-howto-complex-data-types?tabs=portal
        #  https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/search/azure-search-documents#creating-an-index
        return SearchFieldDataType.String
    elif dss_storage_type == "boolean":
        return SearchFieldDataType.Boolean
    elif dss_storage_type == "double":
        return SearchFieldDataType.Double
    elif dss_storage_type == "float":
        return SearchFieldDataType.Single
    elif dss_storage_type == "bigint":
        return SearchFieldDataType.Int64
    elif dss_storage_type == "int":
        return SearchFieldDataType.Int32
    elif dss_storage_type in ["smallint", "tinyint"]:
        return SearchFieldDataType.Int16

    logging.warning(f"Unknown storage type {dss_storage_type} for metadata column {column_name}, attempting to use SearchFieldDataType.String")
    return SearchFieldDataType.String
